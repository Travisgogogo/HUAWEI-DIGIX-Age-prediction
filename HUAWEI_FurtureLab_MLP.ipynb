{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize libraries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize libraries\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,normalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import  Model\n",
    "from keras.layers import Input,concatenate,Dense\n",
    "import keras.backend as K\n",
    "from keras import activations\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from scipy.sparse import csr_matrix, hstack, coo_matrix\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 502 ms, sys: 143 ms, total: 645 ms\n",
      "Wall time: 643 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "age_test = pd.read_csv('../age_test.csv', header = None, names = ['uId'])\n",
    "age_train = pd.read_csv('../age_train.csv', header = None, names = ['uId','age_group'])\n",
    "data = pd.concat([age_train,age_test], axis = 0,sort=True).reset_index()\n",
    "data.drop(['index'],axis=1,inplace=True)\n",
    "del age_test, age_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_basic_info = pd.read_csv('../user_basic_info.csv',header= None, names=['uId','gender','city','prodName','ramCapacity','ramLeftRation','romCapacity','romLeftRation','color','fontSize','ct','carrier','os'])\n",
    "user_behavior_info = pd.read_csv('../user_behavior_info.csv', header = None, names =  ['uId','bootTimes','AFuncTimes','BFuncTimes','CFuncTimes','DFuncTimes','EFuncTimes','FFuncTimes','FFuncSum'])\n",
    "data = data.merge(user_basic_info).merge(user_behavior_info)\n",
    "del user_basic_info, user_behavior_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 496 ms, sys: 155 ms, total: 651 ms\n",
      "Wall time: 627 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in ['A','B','C','D','E','F']:\n",
    "    data['{}FuncTimes'.format(i)] = round(abs(data['{}FuncTimes'.format(i)]))\n",
    "    \n",
    "data['ramLeftCapacity'] = data['ramCapacity'] * data['ramLeftRation']\n",
    "data['romLeftCapacity'] = data['romCapacity'] * data['romLeftRation']\n",
    "data['romLeftRation'][data.romLeftRation>1] = 1\n",
    "data['ramLeftRation'][data.ramLeftRation>1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 45s, sys: 10.5 s, total: 2min 55s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#usage表中top5000使用量的appId\n",
    "app_usage5000 = pd.read_hdf('./feature/app_usage5000.h5', key='data')\n",
    "user_app_actived = pd.read_csv('./data/user_app_actived.csv', header = None, names =['uId','appId'])\n",
    "app_usage5000.columns=['uId', 'usage_appId']\n",
    "data = data.merge(user_app_actived, on='uId', how='left').merge(app_usage5000, how='left', on='uId')\n",
    "X_app = CountVectorizer(token_pattern='a\\d+',binary=True).fit_transform(data['appId'])\n",
    "X_usage = CountVectorizer(token_pattern='a\\d+', binary=True).fit_transform(data['usage_appId'].fillna('None'))\n",
    "del user_app_actived\n",
    "del app_usage5000\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.7 s, sys: 3.55 s, total: 54.3 s\n",
      "Wall time: 54.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "app_usage5000 = pd.read_hdf('../feature/app_usage5000_top_times.h5', key='data')\n",
    "app_usage5000.columns=['uId', 'usage_appId']\n",
    "data = data.merge(app_usage5000, how='left', on='uId')\n",
    "X_usage = CountVectorizer(token_pattern='a\\d+', binary=True).fit_transform(data['usage_appId'].fillna('None'))\n",
    "del app_usage5000\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_feat = [ 'gender', 'city', 'prodName', 'color', 'fontSize', 'ct', 'carrier', 'os']\n",
    "num_feat = ['ramLeftCapacity','romLeftCapacity', 'ramCapacity', 'ramLeftRation', 'romCapacity', 'romLeftRation', 'bootTimes',\n",
    "            'AFuncTimes', 'BFuncTimes', 'CFuncTimes', 'DFuncTimes', 'EFuncTimes', 'FFuncTimes', 'FFuncSum']\n",
    "\n",
    "X_num = RobustScaler().fit_transform(data[num_feat].fillna(0))\n",
    "\n",
    "for feat in cate_feat:\n",
    "     data[feat] = LabelEncoder().fit_transform(data[feat].fillna('None').apply(str))\n",
    "\n",
    "X_cate = OneHotEncoder().fit_transform(data[cate_feat].fillna(-1))\n",
    "#X1 = hstack((X_app, X_cate, X_num), format='csr')\n",
    "X2 = hstack((X_usage, X_cate, X_num), format='csr')\n",
    "del X_cate, X_num\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: train shape (2010000, 11767), test shape (502500, 11767)\n"
     ]
    }
   ],
   "source": [
    "test_index = list(data[np.isnan(data.age_group)].index)\n",
    "train_index = list(data[~np.isnan(data.age_group)].index)\n",
    "# train_x_app = X1[train_index]\n",
    "# test_x_app  = X1[test_index]\n",
    "train_x_usage = X2[train_index]\n",
    "test_x_usage  = X2[test_index]\n",
    "print('All features: train shape {}, test shape {}'.format(train_x_usage.shape, test_x_usage.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetencoder = LabelEncoder().fit(data[~np.isnan(data.age_group)].age_group)\n",
    "y = targetencoder.transform(data[~np.isnan(data.age_group)].age_group)\n",
    "nclasses = len(targetencoder.classes_)\n",
    "dummy_y = np_utils.to_categorical(y)\n",
    "\n",
    "#X_train_app, X_val_app, y_train_app, y_val_app = train_test_split(train_x_app, dummy_y, test_size=0.02, random_state=42)\n",
    "X_train_usage, X_val_usage, y_train_usage, y_val_usage = train_test_split(train_x_usage, dummy_y, test_size=0.02, random_state=42)\n",
    "#del train_x_app, train_x_usage, dummy_y\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义DNN+FM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Layer): \n",
    "    def __init__(self, output_dim=30, activation=\"relu\",**kwargs): \n",
    "        self.output_dim = output_dim \n",
    "        self.activate = activations.get(activation) \n",
    "        super(FM, self).__init__(**kwargs) \n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.weight = self.add_weight(name='weight',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        self.bias = self.add_weight(name='bias',shape=(self.output_dim,),initializer='zeros',trainable=True) \n",
    "        self.kernel = self.add_weight(name='kernel',shape=(input_shape[1], self.output_dim),initializer='glorot_uniform',trainable=True) \n",
    "        super(FM, self).build(input_shape) \n",
    "        \n",
    "    def call(self, x):\n",
    "        feature = K.dot(x,self.weight) + self.bias\n",
    "        a = K.pow(K.dot(x,self.kernel), 2)\n",
    "        b = K.dot(x, K.pow(self.kernel, 2))\n",
    "        cross = K.mean(a-b, 1, keepdims=True)*0.5\n",
    "        cross = K.repeat_elements(K.reshape(cross, (-1, 1)), self.output_dim, axis=-1) \n",
    "        return self.activate(feature + cross) \n",
    "    \n",
    "    def compute_output_shape(self, input_shape): \n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_app():\n",
    "    # create two models\n",
    "    input1  = Input(shape=(X_train_app.shape[1],))\n",
    "\n",
    "    dense_1 = Dense(32, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(16, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_3 = Dense(8, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    out     = Dense(6,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_usage():\n",
    "    # create two models\n",
    "    input1  = Input(shape=(X_train_usage.shape[1],))\n",
    "\n",
    "    dense_1 = Dense(32, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(16, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_3 = Dense(8, kernel_initializer='normal', activation='relu')(dense_2)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    out     = Dense(6,kernel_initializer='normal', activation='softmax')(dense_3)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet():\n",
    "    # create two models\n",
    "    input1  = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "    dense_1 = Dense(256, kernel_initializer='normal', activation='relu')(input1)\n",
    "    #dense_1 = Dropout(0.6)(dense_1)\n",
    "    dense_2 = Dense(128, kernel_initializer='normal', activation='relu')(dense_1)\n",
    "    #dense_2 = Dropout(0.6)(dense_2)\n",
    "    dense_2_x  = concatenate([dense_1,dense_2])\n",
    "    dense_3 = Dense(64, kernel_initializer='normal', activation='relu')(dense_2_x)\n",
    "    #dense_3 = Dropout(0.6)(dense_3)\n",
    "    dense_3_x  = concatenate([dense_1,dense_2,dense_3])\n",
    "    dense_4 = Dense(7, kernel_initializer='normal', activation='relu')(dense_3_x)\n",
    "    #dense_4 = Dropout(0.6)(dense_4)\n",
    "    dense_4_x  = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
    "    out     = Dense(6,kernel_initializer='normal', activation='softmax')(dense_4_x)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMNet():\n",
    "    # create two models\n",
    "    input1  = Input(shape=(X_train.shape[1],))\n",
    "    #DNN_model_I\n",
    "    dense_1 = Dense(100, kernel_initializer='normal', activation='tanh')(input1)\n",
    "    dense_2 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_1)\n",
    "    dense_3 = Dense(150, kernel_initializer='normal', activation='tanh')(dense_2)\n",
    "    dense_4 = Dense(100, kernel_initializer='normal', activation='tanh')(dense_3)\n",
    "    dense_5 = Dense(64, kernel_initializer='normal', activation='tanh')(dense_4)\n",
    "    #FM_model_II\n",
    "    FM_1    = FM(200)(input1)\n",
    "    FM_2    = FM(64)(FM_1)\n",
    "\n",
    "    x       = concatenate([dense_5,FM_2])\n",
    "    x_tmp   = Dense(32,kernel_initializer='normal', activation='softmax')(x)\n",
    "    out     = Dense(6,kernel_initializer='normal', activation='softmax')(x_tmp)\n",
    "    # Compile model\n",
    "    model = Model(inputs=input1, outputs = out)\n",
    "    model.compile(loss ='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])  #logloss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=baseline_model_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1924/1924 [==============================] - 207s 108ms/step - loss: 1.1642 - acc: 0.4993 - val_loss: 1.0818 - val_acc: 0.5401\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.54015, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 2/20\n",
      "1924/1924 [==============================] - 207s 107ms/step - loss: 1.0578 - acc: 0.5495 - val_loss: 1.0543 - val_acc: 0.5529\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.54015 to 0.55294, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 3/20\n",
      "1924/1924 [==============================] - 207s 107ms/step - loss: 1.0388 - acc: 0.5583 - val_loss: 1.0440 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55294 to 0.55697, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 4/20\n",
      "1924/1924 [==============================] - 207s 107ms/step - loss: 1.0285 - acc: 0.5628 - val_loss: 1.0403 - val_acc: 0.5601\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.55697 to 0.56012, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 5/20\n",
      "1924/1924 [==============================] - 205s 107ms/step - loss: 1.0218 - acc: 0.5658 - val_loss: 1.0366 - val_acc: 0.5602\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.56012 to 0.56017, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 6/20\n",
      "1924/1924 [==============================] - 207s 108ms/step - loss: 1.0166 - acc: 0.5679 - val_loss: 1.0327 - val_acc: 0.5618\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.56017 to 0.56179, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 7/20\n",
      "1924/1924 [==============================] - 206s 107ms/step - loss: 1.0127 - acc: 0.5698 - val_loss: 1.0290 - val_acc: 0.5639\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.56179 to 0.56388, saving model to ./weight_bias/best_epoch_model_app_usage_top_times.hdf5\n",
      "Epoch 8/20\n",
      "1325/1924 [===================>..........] - ETA: 1:04 - loss: 1.0075 - acc: 0.5723"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./weight_bias/best_epoch_model_app_usage_top_times.hdf5',monitor='val_acc',mode='max' ,save_best_only=True,verbose=1,period=1)\n",
    " \n",
    "callback_lists=[checkpoint]\n",
    "\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train_usage, y_train_usage, 1024, True),\n",
    "                         nb_epoch=20,\n",
    "                         samples_per_epoch=np.ceil(X_train_usage.shape[0]/1024) ,\n",
    "                         validation_data=(X_val_usage.todense(), y_val_usage), verbose=1,\n",
    "                         shuffle=True,\n",
    "                         callbacks=callback_lists\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=baseline_model_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1924/1924 [==============================] - 233s 121ms/step - loss: 1.1621 - acc: 0.4998 - val_loss: 1.0832 - val_acc: 0.5392\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53923, saving model to ./weight_bias/best_epoch_model_usage.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./weight_bias/best_epoch_model_usage.hdf5',monitor='val_acc',mode='max' ,save_best_only=True,verbose=1,period=1)\n",
    " \n",
    "callback_lists=[checkpoint]\n",
    "\n",
    "\n",
    "fit= model.fit_generator(generator=batch_generator(X_train_usage, y_train_usage, 1024, True),\n",
    "                         nb_epoch=20,\n",
    "                         samples_per_epoch=np.ceil(X_train_usage.shape[0]/1024) ,\n",
    "                         validation_data=(X_val_usage.todense(), y_val_usage), verbose=1,\n",
    "                         shuffle=True,\n",
    "                         callbacks=callback_lists \n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#保存X_app权重\n",
    "model = load_model('./weight_bias/best_epoch_model_app.hdf5')\n",
    "weight = model.get_weights()[0]\n",
    "bias = model.get_weights()[1]\n",
    "weight = weight[:X_app.shape[1],:]\n",
    "np.save('./weight_bias/X_app_weight_dense1.npy',weight)\n",
    "np.save('./weight_bias/X_app_bias_dense1.npy',bias)\n",
    "weight = model.get_weights()[2]\n",
    "bias = model.get_weights()[3]\n",
    "np.save('./weight_bias/X_app_weight_dense2.npy',weight)\n",
    "np.save('./weight_bias/X_app_bias_dense2.npy',bias)\n",
    "weight = model.get_weights()[4]\n",
    "bias = model.get_weights()[5]\n",
    "np.save('./weight_bias/X_app_weight_dense3.npy',weight)\n",
    "np.save('./weight_bias/X_app_bias_dense3.npy',bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#保存X_usage权重\n",
    "model = load_model('./weight_bias/best_epoch_model_app_usage_top_times.hdf5')\n",
    "weight = model.get_weights()[0]\n",
    "bias = model.get_weights()[1]\n",
    "weight = weight[:X_usage.shape[1],:]\n",
    "np.save('./weight_bias/X_usage_weight_times_dense1.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_times_dense1.npy',bias)\n",
    "weight = model.get_weights()[2]\n",
    "bias = model.get_weights()[3]\n",
    "np.save('./weight_bias/X_usage_weight_times_dense2.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_times_dense2.npy',bias)\n",
    "weight = model.get_weights()[4]\n",
    "bias = model.get_weights()[5]\n",
    "np.save('./weight_bias/X_usage_weight_times_dense3.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_times_dense3.npy',bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#保存X_usage权重\n",
    "model = load_model('./weight_bias/best_epoch_model_usage.hdf5')\n",
    "weight = model.get_weights()[0]\n",
    "bias = model.get_weights()[1]\n",
    "weight = weight[:X_usage.shape[1],:]\n",
    "np.save('./weight_bias/X_usage_weight_dense1.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_dense1.npy',bias)\n",
    "weight = model.get_weights()[2]\n",
    "bias = model.get_weights()[3]\n",
    "np.save('./weight_bias/X_usage_weight_dense2.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_dense2.npy',bias)\n",
    "weight = model.get_weights()[4]\n",
    "bias = model.get_weights()[5]\n",
    "np.save('./weight_bias/X_usage_weight_dense3.npy',weight)\n",
    "np.save('./weight_bias/X_usage_bias_dense3.npy',bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Final prediction\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "age_test = pd.read_csv('../age_test.csv', header = None, names = ['uId'])\n",
    "print(\"# Final prediction\")\n",
    "scores = model.predict(test_x)\n",
    "test_label = targetencoder.inverse_transform(np.argmax(scores, axis=1))\n",
    "result = pd.DataFrame()\n",
    "result['id'] = age_test.uId\n",
    "result['label'] = test_label.astype(int)\n",
    "result.to_csv('./submission.csv', index=False)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
